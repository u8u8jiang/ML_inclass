{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Table of contents**  \n[1. Introduction](#intro)  \n[2. Data Preprocessing and cleaning](#data_preprocessing)  \n[3. Temporal Feature Analysis](#temporal_analysis)  \n[4. Individual Feature Analysis](#individual_analysis)  \n[5. Feature Engineering](#feature_engineering)  \n[6. Final Dataset](#finaldataset)  \n[7. Regression](#regression)"},{"metadata":{"_uuid":"c285f750-766a-4020-b453-246a2d5e4995","_cell_guid":"faa7592e-b7b4-4334-8fae-6319ffcf55e1","trusted":true,"scrolled":false},"cell_type":"code","source":"!pip install -U seaborn # seaborn>=0.11","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2be90643-e21b-4c49-911b-6d907f98a7f4","_cell_guid":"94e511f5-e354-496a-9abc-af581b37b78f","trusted":true},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n# 1. Introduction\nThis is a time-series problems, so data presents an order.  Time-based features are probably crucial information for achieveing a good performance. We may want also investigate features based on seasonality, as this is a sales dataset."},{"metadata":{"_uuid":"7be0ca39-4bf0-4145-9b77-d394cc6dda5b","_cell_guid":"3298a2d7-2fb7-496a-9950-510cba2e1b66","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize']=(23,8)\ndef twosubplots(figsize=(23,8)):\n    return plt.subplots(1,2,figsize=figsize)[1]\n\n!ls /kaggle/input/*\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets load dataset. I have got a dataset where item categories are translated to english."},{"metadata":{"_uuid":"71424083-7ad1-4068-8f94-e06c98e96158","_cell_guid":"70bbc3f7-57a1-4b48-a5b8-32389226b3f4","trusted":true,"scrolled":false},"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/competitive-data-science-predict-future-sales'\nROOT_DIR_EN = '/kaggle/input/predict-future-sales-supplementary'\nD = pd.read_csv(ROOT_DIR+ '/sales_train.csv')\nDtest = pd.read_csv(ROOT_DIR+ '/test.csv')\nD_itemcategory = pd.read_csv(ROOT_DIR_EN+'/item_category.csv')\n\nprint(\"train dataset has %d rows\" % len(D))\nD=D.drop('date',axis=1) # dropping date column.\n#D = pd.merge(D, D_item[['item_id','item_category_id', 'item_name']], on='item_id')\nD = pd.merge(D,D_itemcategory, on='item_id')\nD = D.rename(columns={'item_name_translated':'item_name'})\n\nDtest = pd.merge(Dtest,D_itemcategory, on='item_id')\nDtest = Dtest.rename(columns={'item_name_translated':'item_name'})\nDtest['date_block_num']=34\n\ndisplay(D.head(3))\ndisplay(Dtest.head(3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5458cc6e-91f5-43a2-8275-70b018587fbe","_cell_guid":"5095aeb0-9cc5-4003-a5df-f10a16ba4659","trusted":true},"cell_type":"markdown","source":"<a id=\"data_preprocessing\"></a>\n# 2. Data Preprocessing, cleaning and Outliers removal\nLets see the data first"},{"metadata":{"_uuid":"2094c1e2-0e74-4453-8471-99bbb828d930","_cell_guid":"2df58328-722c-4cb5-a4f5-0b69027da418","trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas_profiling # library for automatic EDA\nreport = pandas_profiling.ProfileReport(D)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acbdd8db-1aeb-475e-b37f-050e8c117480","_cell_guid":"3ded1cc6-eb79-4fc5-bc70-4560f13152da","trusted":true,"scrolled":false},"cell_type":"code","source":"display(report)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd2ace4-27f9-48ea-93fb-5ff5671f88cf","_cell_guid":"771b5572-80b6-417c-a8e4-431a5287f9fd","trusted":true},"cell_type":"markdown","source":"* There are no missing values.\n* item_price with negatives values. This represents returned items or something like that.\n* There are only 60 shops ids.\n* Item_price and item_cnt_day probably have outliers."},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Data cleaning\nLets see which items ids, categories ids are common in both train and test dataset."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"D_category_dup = D_itemcategory[D_itemcategory['item_name_translated'].duplicated(keep=False)]\nD_category_dup.sort_values('item_name_translated')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 20 duplicated names. I will use the one with the lowest item_id."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dftmp = D_category_dup.groupby('item_name_translated')['item_id'].min()\nitem_id_mapping = {itemid:dftmp[iname] for _,itemid,iname in D_category_dup[['item_id','item_name_translated']].itertuples()}\nD=D.replace({'item_id':item_id_mapping})\nDtest=Dtest.replace({'item_id':item_id_mapping})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maybe there are item_ids present in the test set but not in the train dataset. Lets see what are they:"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"Duniq = D['item_id'].unique()\nDtest_uniq = Dtest['item_id'].unique()\n\nitem_id_inter = np.intersect1d(Duniq,Dtest_uniq)\n\nlen(item_id_inter),len(Duniq),len(Dtest_uniq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"new_items_test = D_itemcategory[D_itemcategory['item_id'].isin(np.setdiff1d(Dtest_uniq,Duniq))]\nnew_items_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most items on test dataset are in train dataset. There are 361 new items in test dataset that are not present in train dataset."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.histplot(new_items_test, x='item_cat1');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of it really seems to be new products launched only the month of the test dataset. I need to remember this later on the feature engineering phase. "},{"metadata":{"_uuid":"f7b1f883-abf6-4ba7-b94d-2c54a83d3efa","_cell_guid":"704985fc-c74f-49be-a4d6-a9e3174586e4","trusted":true},"cell_type":"markdown","source":"There are no missing values, but the histogram for item_price is really strange. This will be investigated.\n## 2.2 item_price"},{"metadata":{},"cell_type":"markdown","source":"Its seens reasoable to put the average item price."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"Dgtmp = D.groupby(['item_id'])['item_price'].mean()\nDgtmp.name='avg_item_price'\nD = pd.merge(D,Dgtmp, on='item_id')\ndisplay(D.head(3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4308187e-5018-4792-b1a5-2ca669d83302","_cell_guid":"54887ebb-fbd8-4ee4-8519-ecc915560a8a","trusted":true,"scrolled":false},"cell_type":"code","source":"sns.boxplot(y=D['item_price']);\nD['item_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdd80197-0039-41b4-b315-73d77a72e77a","_cell_guid":"ff3625ad-e187-4283-9299-611412a4bc6a","trusted":true},"cell_type":"markdown","source":"There is a single row with negative value for item_price. Also, there is a huge value for item_price. Lets remove first this negative value and then proceed to investigate about the huge value."},{"metadata":{"_uuid":"957eebfe-cd70-40d1-ba44-0dbe3ff6a044","_cell_guid":"a7c904a9-edf0-47cc-a3d2-b44313a3fe93","trusted":true,"scrolled":false},"cell_type":"code","source":"D=D[D['item_price']>0]\nD[D['item_price']>300000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the average price for item id 6066?"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"D[D['item_id']==6066]['item_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is only one item of this type. I am getting rid of it."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"D=D[D['item_price']<300000];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seens reasoable to check if there are items sold much more above average price:"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"D['item_price_relative'] = D['item_price']/D['avg_item_price']\nsns.boxplot(y=D['item_price_relative']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, there items being sold by more than 10 times the average price! Lets investigate what those items are."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dftmp = D[D['item_price_relative']>=10].sort_values(by='item_price_relative', ascending=False)\ndftmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eac25f2a-8063-49c8-a04c-c1f177061ed3","_cell_guid":"3ce52356-885e-4ef4-8a4a-add87006a19b","trusted":true},"cell_type":"markdown","source":"## 2.3 Data preparation\nTest dataset is concerned with month item solds, not days. We will process train data in order to become similar to test dataset. We will lose day information, but will insert it later somehow, if necessary."},{"metadata":{"_uuid":"e65fbddd-a2d1-46b9-93a7-d833a1df5929","_cell_guid":"2da12118-f2aa-4ef1-b5af-9fecd08d5c26","trusted":true,"scrolled":false},"cell_type":"code","source":"Dgroup = D.groupby(['date_block_num','shop_id','item_id','item_cat1','item_cat2'])\nD = Dgroup.agg({'item_cnt_day':'sum',\n                 'item_price':'mean'})\nD = D.rename({'item_cnt_day':'item_cnt_month'}, axis=1).reset_index()\nDgroup = D.groupby('item_id')['item_price'].mean()\nDgroup.name = 'avg_item_price'\nD = D.merge(Dgroup, on='item_id')\nD.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D['item_price_relative'] = D['item_price']/D['avg_item_price']\nD.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ce3d01-912c-4883-901c-f12188d124e2","_cell_guid":"57b65652-492e-4661-b23e-636fee42ddb2","trusted":true,"scrolled":false},"cell_type":"code","source":"sns.boxplot(y=D['item_price_relative']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dftmp = D[D['item_price_relative']>=8].sort_values(by='item_price_relative', ascending=False)\ndftmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9c8ac76-90a1-4492-9468-fbe5a4a83fe9","_cell_guid":"b634274a-9de3-4358-9a04-42d0f93da9b6","trusted":true},"cell_type":"markdown","source":"* item_price: the price of a specific item and specific shop for that month.\n* alltime_avg_item_price: The average price of a specific item, averaged over all months and shops.\n* item_cnt_month: The target variable."},{"metadata":{"_uuid":"1dc19d54-8a83-400d-b928-fba0239ed0cc","_cell_guid":"4ad39233-e996-4b26-bcdd-8a289a62e250","trusted":true},"cell_type":"markdown","source":"<a id=\"temporal_analysis\"></a>\n# 3. Temporal Feature Analysis\n## 3.1 item_cnt_month x time"},{"metadata":{"_uuid":"e6c0957e-f62b-46be-8805-be0e2c4ae40a","_cell_guid":"f3ac5cc2-2c77-47a6-8570-308d000d3571","trusted":true},"cell_type":"markdown","source":"## date_block_num\nLooking the histogram of date_block_num, we can clearly see a seasonality (12 months period) on the number of items sold on a month. Its seens to be in December. Lets investigate:"},{"metadata":{"_uuid":"13bc88b5-b26a-46f4-9696-f5a2c36c2522","_cell_guid":"aee656d4-a7c0-434d-b671-178d7b34a817","trusted":true,"scrolled":false},"cell_type":"code","source":"ax1,ax2 = twosubplots()\ndftmp=D.groupby(['date_block_num']).sum()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_cnt_month', ax=ax1);\ndftmp=D.groupby(['date_block_num']).mean()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_cnt_month', ax=ax2);\nax2.set_ylabel(\"Average item_cnt_month\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5884863d-3ad3-41c2-9c30-243ea3d97358","_cell_guid":"492f6bd0-a510-4127-9885-4272f39a9042","trusted":true},"cell_type":"markdown","source":"In the graph we see a seasonality and a decreasing trend on the total number of items sold per month. However, there is no trend in the average number of itens sold per row."},{"metadata":{"_uuid":"0fd49483-309b-4174-b191-8f5033027417","_cell_guid":"f8c8203f-7676-45b9-9306-c285202efe0d","trusted":true},"cell_type":"markdown","source":"## item_price x time"},{"metadata":{"_uuid":"4ee5d866-d260-4e3d-8854-4932686c3868","_cell_guid":"bbb995f4-2e45-4597-9d28-cbd24d28ab36","trusted":true,"scrolled":false},"cell_type":"code","source":"ax1,ax2 = twosubplots()\ndftmp=D.groupby(['date_block_num']).sum()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_price', ax=ax1);\nax1.set_ylabel('Total Money Spent')\ndftmp=D.groupby(['date_block_num']).mean()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_price', ax=ax2);\nax2.set_ylabel('Average item_price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Average item price is inscreasing with time while the total money spent remains the same (except November,December and January). Maybe people are buying more expensive stuff. We will see this later."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 shop\nLets see the two shops with most items sold in each month"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftmp = D.groupby(['date_block_num','shop_id'])[['item_cnt_month']].sum().sort_values(['date_block_num','item_cnt_month'], ascending=False) #.groupby('date_block_num').max()\n#dftmp = dftmp[dftmp.groupby('date_block_num').idxmax()].reset_index()\ndftmp = dftmp.reset_index().groupby(['date_block_num']).nth([0,1]).reset_index()\ndftmp = D[D['shop_id'].isin(dftmp['shop_id'].unique())]\ndftmp = dftmp.groupby(['date_block_num','shop_id'])['item_cnt_month'].sum().reset_index()\nsns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='shop_id');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see shops 25,28,31 and 54 sells a lot. Shop 31 is the shop with most item_counts, except in the last month where shop 25 sold a little more. Note that shop 25 is almost always the second shop with most sold items.\nInterestingly, shop 54 dissapeared after month 27."},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Item_id\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftmp = D.groupby(['date_block_num','item_id'])[['item_cnt_month']].sum().sort_values(['date_block_num','item_cnt_month'], ascending=False) #.groupby('date_block_num').max()\n#dftmp = dftmp[dftmp.groupby('date_block_num').idxmax()].reset_index()\ndftmp = dftmp.reset_index().groupby(['date_block_num']).nth(0).reset_index()\nmost_sold_items = dftmp['item_id'].unique()\ndftmp = D[D['item_id'].isin(most_sold_items)]\ndftmp = dftmp.groupby(['date_block_num','item_id'])['item_cnt_month'].sum().reset_index()\nsns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='item_id');\n#sns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='item_id');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_itemcategory[D_itemcategory['item_id'].isin(most_sold_items)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"individual_analysis\"></a>\n# 4. Individual feature analysis\nLets start by analysing individual information of each feature, such as the data type, unique values, means, max, etc.."},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Item_price"},{"metadata":{"_uuid":"a80aa5f6-7d05-4aad-bb0b-274e97ae4808","_cell_guid":"e8ae5854-e583-4cef-b187-12b8177d3c04","trusted":true},"cell_type":"markdown","source":"Items below the average price and/or low price, are probably more sold at once. Lets investigate:"},{"metadata":{"_uuid":"c5a32fe5-a548-4d6c-9462-8098a1ba1f9c","_cell_guid":"a84c241b-ae0f-4515-b5b3-ad628bfe6c24","trusted":true},"cell_type":"code","source":"ax1,ax2 = twosubplots()\nsns.scatterplot(data=D, x='item_price_relative', y='item_cnt_month', ax=ax1);\nsns.scatterplot(data=D, x='item_price', y='item_cnt_month', ax=ax2);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9627d5c1-6151-4de5-b458-4645a253f390","_cell_guid":"df07a7d6-a79e-432e-87f4-c4c29d2decf5","trusted":true},"cell_type":"markdown","source":"As it can been seen, items with high item_cnt_month have low price low relative price. However, it is really strange that item_price_relative can have values like 10 or above."},{"metadata":{"_uuid":"4e97b862-4cbc-4ac0-aa79-de5bdfce6dd7","_cell_guid":"bb7b6622-d9a1-4ec3-a1b8-ef7a1126e943","trusted":true,"scrolled":false},"cell_type":"code","source":"#sns.histplot(data=D,x='item_price_relative', hue=pd.cut(D['item_cnt_month'], bins=[-24,0,1,2,3,5,128,2000]));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most item sold have item_cnt_month high and at a item_price_relative=1. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"feature_engineering\"></a>\n# 5 Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#D.to_csv('data_train1.csv') # backup\n#Dtest.to_csv('data_test1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.read_csv('data_train1.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndftmp=None; Dgroup=None\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatenating training dataset with test dataset so feature engineering is done on both at the same time."},{"metadata":{"trusted":true},"cell_type":"code","source":"D = D.append(Dtest, ignore_index=True)\ntrain_idxs = D['date_block_num']<=33\nlen(D)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D['item_cat2']=D['item_cat2'].fillna('na')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clip item counts to (0,20) due to the fact the test dataset is already clipped to (0,20)"},{"metadata":{"trusted":true},"cell_type":"code","source":"D['item_cnt_month'].clip(0,20, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1 Date features"},{"metadata":{},"cell_type":"markdown","source":"Adding month, year and number of days in month as features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from calendar import monthrange\nD['month'] = D['date_block_num'] % 12\nD['year'] = D['date_block_num']// 12 + 2013\nD['num_days_month'] = D.apply(lambda x:monthrange(x['year'], x['month']+1)[1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 All-time based features\nThese features are made based on all months.\n\n### 5.1.1 Absolute item count and item price for each group"},{"metadata":{"trusted":true},"cell_type":"code","source":"group_keys =[['shop_id'],\n            ['item_id'],\n            ['item_cat1'],\n            ['item_cat2'],\n            ['shop_id','item_cat1'],\n            ['shop_id','item_cat2'],\n            ['shop_id','item_id']]\n\nnew_alltimefeatures_names = ['shop', 'itemid','itemcat1','itemcat2','shop-itemcat1','shop-itemcat2', 'shop-itemid']\nnew_alltimefeatures_cnt_names = [\"alltime_%s_cnt\" % name for name in new_alltimefeatures_names] # inserts a prefix and a suffix\nnew_alltimefeatures_price_names = [\"alltime_%s_avgprice\" % name for name in new_alltimefeatures_names] # inserts a prefix and a suffix\n\n\nfor name,k in zip(new_alltimefeatures_cnt_names,group_keys):\n    dftmp = D.groupby(k)['item_cnt_month'].mean()#sum()\n    dftmp.name = name\n    D = D.merge(dftmp, on=k, how='left')\n    \nfor name,k in zip(new_alltimefeatures_price_names,group_keys):\n    dftmp = D.groupby(k)['item_price'].mean()\n    dftmp.name = name\n    D = D.merge(dftmp, on=k, how='left')\nD[['shop_id','item_id']+new_alltimefeatures_cnt_names+new_alltimefeatures_price_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Time-series based features or lag features\n### 5.2.1 First time appeared \nItems at launch date are more sold."},{"metadata":{"trusted":true},"cell_type":"code","source":"Itemfirst = D.groupby(['item_id'])['date_block_num'].min()\nItemfirst.name = 'item_id_first_time'\nD = pd.merge(D,Itemfirst,on='item_id')\nD['item_id_first_time'] = D['date_block_num']-D['item_id_first_time']\n#sns.boxplot(D['item_id_first_time_lag']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.2 Month Lag Features"},{"metadata":{},"cell_type":"markdown","source":"Features based on the past months."},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_keys = [[],\n            ['shop_id'],\n            ['item_id'],\n            ['item_cat1'],\n            ['item_cat2'],\n            ['shop_id','item_cat1'],\n            ['shop_id','item_cat2']]\n\nnew_monthfeatures_names = ['','shop', 'itemid','itemcat1','itemcat2','shop-itemcat1','shop-itemcat2']\nnew_monthfeatures_cnt_names = [\"month_%s_cnt\" % name for name in new_monthfeatures_names] # inserts a prefix and a suffix\nnew_monthfeatures_price_names = [\"month_%s_avgprice\" % name for name in new_monthfeatures_names] # inserts a prefix and a suffix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAGS = [1,2,3,12]\nD.sort_values('date_block_num', inplace=True)\n\nlag_features = []\nfor feature_name,k in zip(new_monthfeatures_cnt_names,cnt_keys):\n    for lag in LAGS:\n        dftmp = D.groupby(k+['date_block_num'])['item_cnt_month'].mean()#.sum()\n        if(len(k)==0):\n            dftmp = dftmp.shift(lag)\n        else:\n            dftmp = dftmp.groupby(k).shift(lag)\n        dftmp.name = '%s-lag%d' % (feature_name,lag)\n        lag_features.append(dftmp.name)\n        D = D.merge(dftmp,how='left',on=k+['date_block_num'])\n        \nfor feature_name,k in zip(new_monthfeatures_price_names,cnt_keys):\n    for lag in LAGS:\n        dftmp = D.groupby(k+['date_block_num'])['item_price'].mean()\n        if(len(k)==0):\n            dftmp = dftmp.shift(lag)\n        else:\n            dftmp = dftmp.groupby(k).shift(lag)\n        dftmp.name = '%s-lag%d' % (feature_name,lag)\n        lag_features.append(dftmp.name)\n        D = D.merge(dftmp,how='left',on=k+['date_block_num'])\n    \nD[(D['shop_id']==24) & (D['item_id']==32)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.3 Relative features\nFeatures that are the proportion/ration of two features."},{"metadata":{"trusted":true},"cell_type":"code","source":"prices_features_lag = [fname for fname in lag_features if 'price' in fname and 'item' in fname]\nprices_features_lag_relative_names = ['%s_relative' % fname for fname in prices_features_lag]\nfor fname,new_fname in zip(prices_features_lag,prices_features_lag_relative_names):\n    D[new_fname] = D[fname]/D['alltime_itemid_avgprice']\nD.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\nD['item_cat1'] = LE.fit_transform(D['item_cat1'])\nD['item_cat2'] = LE.fit_transform(D['item_cat2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"finaldataset\"></a>\n# 6. Final dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = ['date_block_num','month','year','num_days_month','item_cnt_month','item_id_first_time','item_cat1','item_cat2','ID']\nfinal_features += lag_features\nfinal_features += new_alltimefeatures_cnt_names + new_alltimefeatures_price_names\nfinal_features += prices_features_lag_relative_names\nfinal_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling NA values and saving data. The first three dates (date_block_num<3) are not saved, since not enough lag time feature is available."},{"metadata":{"trusted":true},"cell_type":"code","source":"D=D[final_features]\nD=D.fillna(0)\nDtrain=D[(D['date_block_num']<=33) & (D['date_block_num']>=3)]\nassert(len(D[D['date_block_num']==34]) == len(Dtest))\nDtest = D[D['date_block_num']==34]\n#Dtrain.to_csv('date_train_final.csv', index=False)\n#Dtest.to_csv('date_test_final.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"regression\"></a>\n# 7. Regression"},{"metadata":{},"cell_type":"markdown","source":"Separating features from target label:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dtrain=Dtrain[Dtrain['date_block_num']<=7]\nX = Dtrain.drop(['item_cnt_month','ID'], axis=1)\nY = Dtrain['item_cnt_month']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sampler to be used in time datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TimeSplitter:\n    def __init__(self, n_splits=1):\n        self.n_splits=n_splits\n        \n    def split(self,X,y=None,groups=None):\n        date_block_num = X['date_block_num']\n        max_date = date_block_num.max()\n        for i in range(self.n_splits):\n            test_date=max_date-i\n            train_idxs, = np.where(date_block_num<test_date)\n            test_idxs, = np.where(date_block_num==test_date)\n            yield train_idxs,test_idxs\n        \n    def get_n_splits(self,X=None,y=None,groups=None):\n        return self.n_splits\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking algorithm. Did not worked for me quite as well as Xgboost."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, clone\nfrom sklearn.linear_model import LinearRegression\n\nclass TimeStackingRegressor(BaseEstimator):\n    def __init__(self, estimators, final_estimator=LinearRegression(), n_splits=1, passthrough=False):\n        self.estimators=estimators\n        self.final_estimator = final_estimator\n        self.n_splits = n_splits\n        self.passthrough=passthrough\n        \n    def fit(self, X,y):\n        timesplitter = TimeSplitter(self.n_splits)\n        Xlvl2=[]\n        Xlvl1=[]\n        ylvl2=[]\n        for train_idxs, test_idxs in timesplitter.split(X):\n            preds = np.empty((len(test_idxs),len(self.estimators)))\n            Xtrain,ytrain = X.values[train_idxs], y.values[train_idxs]\n            Xtest,ytest = X.values[test_idxs], y.values[test_idxs]\n            for i,(name,estimator) in enumerate(self.estimators):\n                estimator = clone(estimator)\n                estimator.fit(Xtrain,ytrain)\n                preds[:,i]=estimator.predict(Xtest)\n            Xlvl2.append(preds)\n            Xlvl1.append(Xtest)\n            ylvl2.append(ytest)\n        Xlvl1 = np.vstack(Xlvl1)\n        Xlvl2 = np.vstack(Xlvl2)\n        ylvl2 = np.hstack(ylvl2)\n        if(self.passthrough):\n            Xlvl2 = np.hstack((Xlvl1,Xlvl2))\n        self.final_estimator.fit(Xlvl2, ylvl2)\n        \n        for name, estimator in self.estimators:\n            estimator.fit(X,y)\n        \n        return self\n    \n    def predict(self, X):\n        preds = np.empty((len(X),len(self.estimators)))\n        for i,(name,estimator) in enumerate(self.estimators):\n            preds[:,i] = estimator.predict(X)\n        if(self.passthrough):\n            Xlvl2 = np.hstack((X,preds))\n        else:\n            Xlvl2 = preds\n        return self.final_estimator.predict(Xlvl2)\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Constructs all classifiers. Various classifiers were tested, but Random Forest and Xgboost are the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_validate, PredefinedSplit, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm\nfrom tqdm import tqdm\nimport time\nfrom sklearn.feature_selection import SelectPercentile, f_regression\nimport xgboost\n\nRANDOM_STATE=42\n\ndef createClassifiers():\n    clfs = []\n    lr = Pipeline([('feature_selector',SelectPercentile(f_regression, percentile=70)),\n                  ('lr',LinearRegression(n_jobs=-1))])\n    knn = KNeighborsRegressor(3, n_jobs=-1)\n    knn = Pipeline([('feature_selector',SelectPercentile(f_regression, percentile=50)),\n                    ('scaler',StandardScaler()),\n                    ('clf',knn)])\n    dt = DecisionTreeRegressor(min_impurity_decrease=0.001)\n    rf = RandomForestRegressor(n_estimators=100, max_features=0.5, min_impurity_decrease=0.001,\n                               criterion='mse', n_jobs=-1, random_state=RANDOM_STATE) #best_params for random forest{'max_features': 0.5, 'min_impurity_decrease': 0.001}\n    rf2 = GridSearchCV(RandomForestRegressor(n_estimators=100, criterion='mse',min_impurity_decrease=0.001, n_jobs=-1, random_state=RANDOM_STATE),\n                       {'max_features':[0.4,0.5,0.6]}, cv=TimeSplitter())\n    gbreg = GradientBoostingRegressor(n_estimators=50, learning_rate=0.15, min_impurity_decrease=0.001)\n    \n    \n    stack1_estimators= [('lr',lr),\n                        ('rf',rf)]\n    #stack1_reg = TimeStackingRegressor(estimators=stack1_estimators, n_splits=5)\n    xgb = xgboost.XGBRegressor(max_depth = 10, n_estimators=100, min_child_weight=200, subsample = 1, eta = 0.5, seed = RANDOM_STATE)\n\n    #clfs.append(('knn',knn))\n    #clfs.append(('dt',dt))\n    #clfs.append(('stacking1',stack1_reg))\n    #clfs.append(('rf',rf))\n    #clfs.append(('gbreg',gbreg))\n    #clfs.append(('rf2',rf2))\n    #clfs.append(('knn',knn))\n    clfs.append(('xgb',xgb))\n    clfs.append(('lr',lr))\n    \n    return clfs\n\n\nclfs = createClassifiers()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Training\nTrains a model **if necessary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nimport pickle\n\nbest_model = None\nif(os.path.isdir('/kaggle/input/my-modelpkl')):\n    with open('/kaggle/input/my-modelpkl/best_model.pkl','rb') as f:\n        best_model = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif(best_model is None):\n    sampler = TimeSplitter()\n    Results={}\n    pbar = tqdm(clfs)\n    for clf_name,clf in pbar:\n        pbar.set_description(\"Training %s\" % clf_name)\n        Results[clf_name] = cross_validate(clf, X, Y, cv=sampler, scoring='neg_root_mean_squared_error', return_estimator=True, return_train_score=True)\n    best_model = Results['xgb']['estimator'][0]\n    \n    R = []\n    for clf_name,res in Results.items():\n        R.append([clf_name,'mse', -res['test_score'][0], -res['train_score'][0]])\n    R = pd.DataFrame(R, columns=['classifier name','metric','test_rms', 'train_rms'])\n    display(R)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\n_, ax = plt.subplots(1,1,figsize=(10,14))\nplot_importance(booster=best_model, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"Dtest['item_cnt_month'] = best_model.predict(Dtest.drop(['item_cnt_month','ID'],axis=1))\nDtest = Dtest.sort_values('ID')\nDtest['ID'] = Dtest['ID'].astype(int)\nDtest['item_cnt_month'] = Dtest['item_cnt_month'].clip(0,20)\nDtest.to_csv('submission.csv', columns=['ID','item_cnt_month'], index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}